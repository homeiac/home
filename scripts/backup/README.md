# backup Scripts

> Auto-generated by `scripts/generate-readme.sh` - do not edit manually

## Scripts

| Script | Description |
|--------|-------------|
| `install-rclone.sh` | Install rclone to ~/.local/bin (no sudo required)<br>Usage: ./install-rclone.sh<br>Downloads and installs rclone for the current user.<br>Works in containers and environments without root access. |
| `setup-pbs-external-datastore.sh` | Setup external HDD as a PBS datastore (PBS-native approach)<br>This script:<br>1. Detects and mounts external USB HDD on Proxmox host<br>2. Formats as ext4 if needed (with confirmation)<br>3. Passes through mount to PBS LXC container (103)<br>4. Creates PBS datastore using proxmox-backup-manager<br>Benefits over rsync approach:<br>- Deduplication (saves 50-70% space)<br>- Incremental sync (only changed chunks)<br>- Native PBS restore commands work<br>- Web UI visibility<br>- Integrity verification<br>Run on: pumped-piglet.maas (where PBS LXC 103 and external HDD are) |
| `setup-pbs-sync-job.sh` | Setup PBS sync job to replicate backups to external datastore<br>This script:<br>1. Creates a PBS sync job from homelab-backup to external-hdd<br>2. Configures daily schedule<br>3. Enables deduplication and incremental sync<br>Prerequisites:<br>- setup-pbs-external-datastore.sh has been run<br>- Both datastores exist in PBS<br>Run on: pumped-piglet.maas (where PBS LXC 103 is) |
| `setup-pbs-verify-job.sh` | Setup PBS verify job for backup integrity checking<br>This script:<br>1. Creates a PBS verify job for the main datastore<br>2. Optionally creates verify job for external datastore<br>3. Configures weekly schedule<br>Why verify jobs matter:<br>- Detects bit rot and corruption before you need the backup<br>- Validates all backup chunks are intact<br>- Alerts on corrupted backups via PBS notifications<br>Run on: pumped-piglet.maas (where PBS LXC 103 is) |
| `setup-rclone-gdrive.sh` | Setup rclone with Google Drive for offsite backup<br>This script:<br>1. Checks for rclone (run install-rclone.sh first if needed)<br>2. Guides through Google Drive OAuth setup (supports headless)<br>3. Creates backup sync configuration<br>4. Tests connection<br>For headless/container environments:<br>1. Run this script and choose "headless" mode<br>2. On a machine with a browser, run: rclone authorize "drive"<br>3. Paste the token when prompted |
| `setup-zfs-snapshots.sh` | Setup ZFS automatic snapshots on Proxmox hosts<br>Uses sanoid for snapshot management with configurable retention<br>Targets:<br>- pumped-piglet.maas: local-20TB-zfs pool<br>- fun-bedbug.maas: local-3TB-backup pool<br>Retention: 24 hourly, 7 daily, 4 weekly, 3 monthly |
| `sync-postgres-to-gdrive.sh` | Sync PostgreSQL backups from K8s PVC to Google Drive<br>This script can be run:<br>1. Manually from a node with kubectl and rclone<br>2. As part of a K8s CronJob (see gdrive-sync-cronjob.yaml)<br>Prerequisites:<br>- rclone configured with Google Drive (run setup-rclone-gdrive.sh)<br>- kubectl access to K8s cluster<br>- PostgreSQL backup CronJob running (backup-cronjob.yaml) |
| `test-backup-strategy.sh` | Test backup strategy components<br>Safe to run - no destructive operations<br>Usage: ./test-backup-strategy.sh [--full]<br>--full: Include Level 3 component tests (requires infrastructure access)<br>Don't use set -e as we want to continue on test failures<br>set -e |
| `test-postgres-restore.sh` | Test PostgreSQL restore from backup<br>This script:<br>1. Downloads the latest backup from Google Drive (or uses local)<br>2. Creates a test database<br>3. Restores selected tables/data<br>4. Verifies the restore<br>5. Cleans up<br>Usage: ./test-postgres-restore.sh [--from-gdrive|--from-local]<br>--from-gdrive: Download backup from Google Drive first<br>--from-local: Use existing backup from PVC (default) |

*Generated: 2026-01-18*
